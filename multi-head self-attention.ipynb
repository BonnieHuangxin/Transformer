{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import *\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Dot-Product Attention的实现\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,d_model):\n",
    "        super(ScaledDotProductAttention,self),__init__()\n",
    "            #计算缩放因子为8\n",
    "        self.temper = np.power(d_model,0.5) #d_model为 词嵌入维度512\n",
    "    \n",
    "    def forward(self,Q,K,V):\n",
    "        qk = torch.bmm(Q,K.transpose(1,2))\n",
    "        scaled_qk = qk/self.temper\n",
    "        attention_score = F.softmax(scaled_qk,dim = 1)\n",
    "        V_attention = torch.matmul(V,attention_score)\n",
    "        return V_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutilHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim = 512, num_heads = 8,dropout = 0.0):\n",
    "        super(MutilHeadAttention,self).__init__()\n",
    "        self.per_head_dim = model_dim//num_heads\n",
    "        self.num_heads = num_heads\n",
    "        #线性映射为8份\n",
    "        self.linear_k = nn.Linear(model_dim, self.per_head_dim*num_heads)\n",
    "        self.linear_q = nn.Linear(model_dim, self.per_head_dim*num_heads)\n",
    "        self.linear_v = nn.Linear(model_dim, self.per_head_dim*num_heads)\n",
    "        \n",
    "        #attention\n",
    "        self.attention_net = ScaledDotProductAttention(d_model) #d_model = 64\n",
    "        \n",
    "        #最后拼接输入线性映射层\n",
    "        self.final_linear = nn.Linear(model_dim,model_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # multi-head attention之后需要做layer norm\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "    \n",
    "    def forward(self,query,key,value):\n",
    "        #残差\n",
    "        residual = query\n",
    "        num_heads = self.num_heads\n",
    "        per_head_dim = self.per_head_dim\n",
    "        #batchsize\n",
    "        batch_size = k.size(0)\n",
    "        \n",
    "        # linear projection \n",
    "        key = self.linear_k(key)\n",
    "        query = self.linear_k(query)\n",
    "        value = self.linear_k(value)\n",
    "        \n",
    "        #spilt heads 64维\n",
    "        key = key.view(batch_size*num_heads,-1,per_head_dim)\n",
    "        query = query.view(batch_size*num_heads,-1,per_head_dim)\n",
    "        value = value.view(batch_size*num_heads,-1,per_head_dim)\n",
    "        \n",
    "        #送入attention_net\n",
    "        outputs = self.attention_net(query,key,value)\n",
    "        \n",
    "        #拼接\n",
    "        outputs = outputs.view(batch_size,-1,per_head_dim*num_heads)\n",
    "        \n",
    "        #送入线性层\n",
    "        outputs = self.final_linear(outputs)\n",
    "        #dropout\n",
    "        outputs = self.dropout(outputs)\n",
    "        \n",
    "        #加上残差项，再layer norm\n",
    "        outputs = self.layer_norm(outputs+residual)\n",
    "        \n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
